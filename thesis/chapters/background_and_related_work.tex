\chapter{Background and Related Work}\label{chapter:background_and_related_work}
The Background section of this thesis establishes the foundation for understanding
a schedulers principles, technologies, and practical considerations. 

%First we need to explain the development of multithreading

%We then explain the development of scheduling paradigms 

%then 

\section{Evolution of CPU Architectures}
%my idea would be to start very broadly then focus in on the fact that Moore's Law is now 
%carried by the rise in number of cores. This in turn necesicates?->  
In the last decades, computer performance has become faster with each generation. This trend described by Moore's Law, which predicts that the number of transistors in a dense integrated circuit doubles about every two years. While not a natural law, this industry trend has been observed from the 1970s to the present day, driving significant advancements in computing power and efficiency\cite{Moore2006}.

Initially, lithographical improvements enabled more transistors in a smaller package, directly translating to faster processors. For example, the manufacturing process improved from 180nm (Pentium III in 2000) to 32nm (Westmere in 2010) \cite{Bohr2011}. During this period, clock speeds increased from around 1 GHz to almost 4 GHz. It is also important to note that the effective increase in clock speed is higher due to Instructions per Cycle improvements. 

%%SHIFT TO MULTICORE 

Unfortunately the lithographical improvements were not sustainable, due to the physical properties and manufacturing difficulties in smaller manufacturing processes. As a result the rate of improvement slowed down, the focus shifted to increasing clock speeds and improving IPC. However, this approach faced also physical limitations, primarily due to power consumption and heat generation \cite{Borkar2007}. 

Around 2005, a paradigm shift occurred in CPU design: instead of pushing for higher clock speeds, manufacturers began to increase the number of cores on a single chip. This marked the beginning of the multi-core era, with dual-core processors becoming mainstream and quad-core processors following shortly after \cite{Geer2005}.

CPUs not only developed into multi-core, they were able to do simultaneous multithreading (SMT or also Hyper-Threading). SMT enabled more computing power in the same die. Becuase a single core can execute two threads simultaneously, cpus become more forgiving towards less optimal memory usage. As waiting for memory did not stall the entire core, when a thread is expecting memory. This flexibility allowed easier multiprogramming in the early 2000s. 

Moore's Law continued to be true, but the contributers to the increase in transistors changed. We observe a logarithmic development in clock speeds, power consumption and performance per clock. This means improvements are in logarithmic relation, which is not appropiate for linear increase in Moore's Law. 

After a short stagnation, mostly due to lack of competition, number of cores are increasing steadily. 
Leading to the development of many-core CPUs, particularly for enterprise use cases. These processors boast significantly more cores than consumer processors, with some modern server CPUs featuring 64 or even 128 cores \cite{AMD2021}.
However, this shift introduced new challenges, often referred to as the "multicore crisis" \cite{Sutter2005}. Adding more cores to a CPU often necessitated a decrease in clock speeds due to power and thermal constraints. Even today's many-core CPUs typically have clock speeds about 1-2 GHz lower than their consumer counterparts. Although, many-core cpus have more sophisticated memory arrangements, often having a decade lead in terms of bandwidth, latency and error correction. For example, latest server cpus have often 8 or 12 channel memory, Error Correction Code (ECC) memory support as well as higher speed interconnects. These advantages allow to saturate the cores.  

%%2.1.4 Memory Hierarchies and Cache Performance
The evolution towards multi-core and many-core architectures brought broader memory bandwidth and more cache per core. 
Still, memory outside of the processor needs to be accessed via the network connection, this operation stalls the core.
Worse yet, the CPU operates considerable faster than the main memory it uses. Cpus found themselves increasingly starved for data, also called Von Neumann Bottleneck. As the portion of memory fethcing in the program execution increases, runtime becomes longer, this is especially true with multithreading. To minimize the time spent waiting, modern CPUs often feature complex cache hierarchies with multiple levels (l1, l2, l3), each with different sizes and access times \cite{hennessy2019}.

Cache performance has become crucial for overall system performance. Memory-intensive applications with suboptimal cache usage are slower, even with multithreading, than implementations with better cache utilization. This is because main memory access is prohibitively expensive compared to cache access, often by an order of magnitude or more \cite{Drepper2007}. Data locality, or cache locality, needs to be cared for for optimal performance. 

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Cache}        & \textbf{Alder Lake P core}                             & \textbf{Alder Lake E core}                             & \textbf{Zen 4} \\ \hline
\textbf{Level 1 code} & 32\,kB       & 64\,kB     & 32\,kB, latency 4 clocks \\ \hline
\textbf{Level 1 data} & 48\,kB, latency 5  & 32\,kB, latency 3  & 32\,kB, latency 4 clocks \\ \hline
\textbf{Level 2}      & 1280\,kB, latency 15 & 2048\,k, latency 20 & 1\,MB, latency 14 clocks \\ \hline
\textbf{Level 3}      & 4–30\,MB, latency 65, shared   & 4–30\,MB, shared               & 32–64\,MB one per 8 cores, latency 47 clocks \\ \hline
\end{tabular}%
\caption{Cache sizes and latencies for Alder Lake (P and E cores) and Zen 4 architectures}
\end{table}
To mitigate this problem, modern processors employ techniques such as prefetching to minimize cache misses. Additionally, cache-oblivious algorithms and data structures can outperform those with better asymptotic runtime but worse cache performance \cite{Frigo1999}.




\subsection{Cache Coherent Non-Uniform Memory Access (NUMA)}
As core counts increased, traditional symmetric multiprocessing (SMP) architectures faced scalability challenges. This led to the development of Non-Uniform Memory Access (NUMA) architectures, where memory access time depends on the memory location relative to the processor \cite{Lameter2013}. NUMA introduces additional complexity to scheduling and memory management, particularly in many-core systems. As shown above, utilizing the cache optimally is key to not incur unncessary memory fetch from the main memory, which can be around 150-200 cycles and even longer if the access was to remote memory. Waiting for data is especially crucial in parallel programming, as waiting or blocking of the execution impacts the whole execution time. Often a simple inefficiency snowballs into a considerable chunk of wasted execution time. 
NUMA enhances memory performance in two ways. Firstly, it reduces the memory latency for recently used data. More importantly, it reduces the number of accesses to the main memory. Therefore NUMA is beneficial for workloads with high memory locality of reference and low/no lock contention. 
A hardware design challenge for NUMA architectures is to preserve coherence between caches of different NUMA clusters. A coherence problem stems from the tiered structure of the Cache itself. Up until Sandy Bridge EP and Bulldozer architectures the Last Level Cache (LLC) was not tiered and shared among all cores. As private cache (L1 and L2) per core emerged, the coherency between the caches needed to be kept. The usual memory coherence protocol MESI can be used, though at this level the communication for the Shared (S) state would be too high on the system.
In a cpu with 64 cores with their respective caches, a read request can in the worst case trigger many cache snoops and their answers from the caches (cachelines) in state S. Modern cache architectures apply the MESIF protocol introducing Forwarding (F) state. One cacheline is promoted to the F state, so it is the only cache that can respond to requests. The F state indicates "first among equals". In the MESIF protocol it reduces the traffic on the rings or on the interconnects. \cite{Goodman} %Goodman JR, Hum, HHJ (2009). MESIF: A Two-Hop Cache Coherency 
                                   %Protocol for Point-to-Point Interconnects. 23 pages.
                                   %http://hdl.handle.net/2292/11594

Current trends are towards heterogeneous unified memory access  (e.g. CPU + GPU) and towards coherent addressing of remote memory. With more compplex memory controllers, a cpu can directly address the memory of an accelerator. Modern designs often incorporate multiple levels of memory. AMD's infinity fabric allows tighter integration of shared memory among its devices (both CPU and GPU), while NVIDIA's CUDA and Intel's OneAPI are developed to give better access to their respective platforms. The goal is to reduce memory related bottlenecks. A cpu needs to request the execution data and communicate with the accelerator, and that accelerator needs to load the memory as well. In the hUMA architectures a cpu can directly load the data related to the execution on the accelerators memory. 

                            
 
%%Branch prediction and Out of order execution and spekulative execution
\subsection{Optimizations}
waiting for data is often the main cause of low performance. To mitigate this problem, modern CPUs employ a plothera of solutions to minimize this time. Most important optimization is prefetching. Prefetching, loads a cacheline (mostly 64 Bytes) into the cache before the region it was loaded from, is requested. This is adventageous as it can be done parallel to another execution and it is computationally cheap. Modern CPUs have built in prefetching. Hardware prefetching is more efficient than explicit software prefetching in most of the cases.\cite{agnerfog}

Out of Order Execution (OOE) is another way to reduce the time spent waiting, by reordering the execution so that independent instructions can be executed while waiting for memory to be fetched. OOE is more suitable for unpredictable access patterns, as prefetching would miss the next access, thus causing overhead. For compute bound workloads OOE is more impactful, where prefetching shines at memory bound workloads.\cite{OOE}

Branch Prediction and Spekulative Execution are both probabilistic methods to select a branch in the execution that is likely to be the correct path in the conditional. The CPU continues to execute and/or prefetch the instructions and data in that path. If the executed path is the wrong path, then the executed pipelines are discarded. This is also called a branch misprediction. The maximum wasted cycles are the length of a pipeline. A branch miss is more likely if the conditional is close to random. If the conditionals result is distributed according to some distribution, the branch prediction is more likely to be correct.\cite{agnerfog} 
 

%2.1.6 Implications for Parallel Programming and Scheduling
\subsection{Implications for Parallel Programming}
The shift to many-core architectures has profound implications for software development and system optimization. The real improvement in computing power now lies less in waiting for faster hardware and more in writing cache-aware, scalable parallel programs that can efficiently utilize anywhere from 16 to thousands of cores \cite{Keckler2011}.
This evolution underscores the critical importance of effective scheduling algorithms, particularly those that can provide low latency in many-core environments. Schedulers must now contend with:
\begin{enumerate}
  \item Increased core counts and potential for parallelism        
  \item Complex cache hierarchies and the need for cache-aware scheduling
  \item NUMA effects and the importance of memory locality
  \item The need to balance high throughput with low latency for diverse workload
\end{enumerate}

In the context of this thesis, understanding these architectural trends is crucial for developing effective low latency scheduling algorithms for many-core CPUs. The challenges posed by increased core counts, complex memory hierarchies, and diverse application requirements form the foundation for our research into advanced scheduling techniques.



\section{Parallel Programming Models}

In this section we will give a brief overview on the current parallel programming models. Parallel programming is necassary for reaping the benefits of hardware improvements. There are many models for parallel programming, but they can be roughly divided into two classes. 

\subsection{Process Interaction}
In the parallel programming terminology, some problems (e.g. embarrasingly parallel problems) do not require communication. These problems can be divided into self contained subtasks. In that case there is no need for process interaction. However, this case is rare in real world tasks where many modes of dependencies need to be communicated between processes. Problems like weather prediction, heat diffusion etc. cannot be calculated in parallel without process interaction. Communication can be done explicitly or implicitly. 

\subsubsection{Shared Memory}
Shared Memory model is the most common way to tackle communication on todays multi-core cpus. It is fast and does not incur communication overhead with remote memory locations. 
Only considering the process communication without threads, processes share a common memory address space that they read and write to asyncronously. In the case of concurrent access, syncronization needs to be implemented to avoid race conditions. Common interactions shape the execution such as waiting, yielding or notifying. Syncronization can be expensive if the blocking of other processes takes too long therefore limiting gains of parallel programming.
Considering threads as well the situation looks similar with one difference: threads share the memory of a single process. This means the threads are not in race condition with other threads of another process. Threads can be user level or kernel level. Changing the execution to another thread is cheap.  


One interesting area of parallel programming is the development of lock-free data structures. In this case concurrent access to the data is not blocking, eliminating the block and wait operations. 
Depending on the memory model more care needs to be taken in order to guarentee correctness of lock-free algorithms. In the strong memory model (sequential consistency model) limits the possible reordering of the memory related to the execution by enforcing memory fences.
Memory fence enforces the order of memory executions. On a high level we can imagine 4 memory barriers:
\begin{itemize}
  \item LoadLoad: prevents reordering of loads performed before the barrier with loads afther the barrier. 
  \item LoadStore: processor is allowed to skip load operations if the coherence still applies afterwards. 
  \item StoreLoad: the latest stored value is visible to all other processors, loads after the barrier receive the correct value. 
  \item StoreStore: prevents the reordering of the stores performed before the barrier with stores after the barrier.
\end{itemize}
Each restricting the reordering of the memory operations. In the strong memory model, all but StoreLoad reordering are prohibited, even though it acts as like all of the barriers combined. This way the compiler and the cpu cannot reorder the execution with more freedom, therefor limiting the possibilities of more relaxed memory operations. 

With the weak memory model it is possible to use all of the reorderings. This way the hardware is released of the burden of implicit acquire and release of the memory (locking the memory essentially).
%%TODO: Maybe talk about consensus? 
Another important concept is the atomic compare-and-swap (CAS) operation. In most modern hardwares CAS operation is implemented on the hardware atomically. Introduced in C++ 11 the compare exchange weak allowing weak CAS operation, thus eliminating the need for a lock. 
Being able to use lock-free memory operations with cohesion guarentees lay the foundation of lock-free algorithms. In the context of parallel programming, not waiting for data is the key. 

In this case the main focus of the scheduler is to schedule threads in a nonblocking manner, therefor it is very important to make use of weak memory model as well. Shared memory ensures low latency to memory access, but for cache locality the scheduler needs to keep the NUMA principle in mind. 

\subsubsection{Message Passing}
In the message passing model, processes have their own private memory, and communication is performed by sending and receiving messages. This model is well-suited for distributed systems, where processes may run on different machines or processors without shared memory. MPI (Message Passing Interface) is a common standard used in this model, which ensures scalability and fault tolerance in large-scale systems via explicit .

Message Passing solves the limitation of scalability in Shared Memory models, though while having more overhead in execution planing. Newest research is in asyncronous communication primitives primarily after MPI-3 using nonblocking collective operations in MPI-3, fault tolerance for exascale computing with User-Level Failure Mitigation (ULFM) proposal for MPI and automatic optimization of hybrid programming models (e.g. MPI + OMP). 

Message Passing schedulers have to face the communication overhead of the model. Selecting the optimal nodes/clusters for communication is a challenge itself. Because scheduling latency is not a big consideration for exascale type of work, the focus is the saturation of computing power as well as data coherence (fault tolerance). 
 
\subsubsection{Partitioned Global Address Space (PGAS)}
PGAS is a hybrid approach combining the shared memory aspect by logically partitioning the distributed memory. The whole memory is adressible to the process without the need for message passing. Although internal implementation uses memory management similar to message passing. This often involves moving memory to caches of the clusters and/or propagating the updates in the system. Popular examples are Chapel\cite{chapel}, X10 and Unified Parallel C. Current research includes more efficient runtime enviroments with cheaper data movements, interoperability with existing frameworks such as MPI, and better scaling for exascale computing. 





\subsection{Problem Decomposition}

A given task 


\subsubsection{Task Level Decomposition}

\subsubsection{Data Level Decomposition}

\subsubsection{Stream and Implicit Parallelization}


\section{Low Latency Scheduling}


\section{Cache Aware Scheduling}
%false sharing 
% 

\section{Concurrent Data Structures for Scheduling}
%Deque
%concurrent Deque
%ABP algo 
%Chase Lev 
%Private and half 


\section{Application Domains for Low Latency Scheduling}



