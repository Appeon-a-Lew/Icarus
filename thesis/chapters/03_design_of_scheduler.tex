\chapter{Design of the scheduler}\label{chapter:design}

\section{Section}
Todo: explain the design choices 
Todo: Graphics
Todo: code maybe


Morsel size: varying the morsel size is crucial.

Types of Deques: simple deque, split in half and hybrid. 
In contrast, Heartbeat cur-
rently supports three load-balancing algorithms: work steal-
ing with concurrent deques, work stealing with private de-
ques (as described in [3 ]), as well as a mixed variant that in-
volves both a concurrent cell for storing the top-most deque
item and a private deque for storing all other items.1 Pre-
liminary experiments suggest that the three variants give
similar results, with a slight advantage for the mixed variant
of work stealing. For this reason, we benchmark Heartbeat
using this mixed variant.


How does uneven mailboing work: 

first: at a contention phase we do not allow for stealing bc a task will be mailed to our mailbox soon. 

second: The main Thread will split its range in two pieces (9 * (n + 1))/16 as the fence so we dont get penalized assoc set caches
The left range will be kept for itselfs and the right will be mailed to another thread. 
Until a grainsize is achieved we will continue to split the range and mail. ceil(Grainsize = range size / numthreads)

when the grainsize ranges are distributed, The tasks i.e. morsels are pushed into the Deque. 
and we can resume with execution with stealing. 

Question do we need explicit mailboxes? 
Probably not, if the spawned threads are constantly trying to steal from each other, if we start splitting and putting it in our Deque
a thread will definately steal it before we can execute our half. This is also outlined in TBB documentation. Their findings suggest 
receiving a job is faster than stealing it, for starting latency at the very least. 

So in my experimental setup we will try to do that explicitly. We will mail the ranges to threads that are waiting for 
work and this will result in improved start latency. 

We can also experiment with uneven splitting that is the idea that an earlier thread splits the job in grainsize at the start and mails them 
until it has a grainsize range to work on. This way the threads overall save the time on signal communications. (atomic wait and wake)

IF this proves to be viable, combined with affinity boost of morsels the scheduling time will be even lower than the native TBB scheduler. 


Another concern can be the use of explicit fork-join parallelism. 
This approach much like mailboxing uses subsequent division of the range and depends on the efficient (random) stealing. 
Such an implementation is the direct comparison to the mailboxing with task pooling (TBB)\dots

Why is it faster? 

In the Work Stealing version the job is divided at the schedule begin, which is an inescapable task. Afterwards all threads start at the same time. 
Visual: 

=======================|execute task
.......................|execute task 
.......................|execute task

vs: 

========|========|========|execute task  
........|execute task
.................|execute task 

This effect is even greater with more number of threads available(given the work is enough to saturate them all else bc of the insufficient jobs the whole 
task would be over before we can reach the last thread. That would be still faster than the first one bc spawning that many threads with that little jobs 
would defeat the purpose of multithreading).


Taken the fact that execute task is long enough initial start latency will be reduced and the earlier threads will start to steal earlier 
This would be detrimental to the runtime of the scheduler if the stealing mechanism was not lockfree. This is why most efficient implementations
of work stealing implement a type of split deque or a work stealing deque. 

Now during the splitting phase it should be clear that any attempt at stealing will yield poor performance. So we can introduce a condition variable 
"can steal" that way we can signal the allowance of stealing after the end of contention phase. This way Threads can wait/sleep/spin and react at the 
sight of a job. This way if more job is acquired we can also introduce another variable "ELASTICITY_ENABLED" if enabled we wake up only one thread waiting
on the morsel. It is more energy efficient (probably) than waking them all up, but we want to minimize the start latency so this option though implemented 
will not be made use of greatly.

The reliance on the atomic variables can greatly impact the runtime of the scheduler as we are depending on the signal send and receive times. 
Sending a signal is almost always faster than receiving a signal. So we can utilize a signal handler to emphasize another tactic of scheduling. 


A signal based implementation uses real interrupts (pthread_kill(tid, SIGUSR1) for example) that are handled by the OS instead of Atomics. 



TBB implementation of mailboxing detailed: 

Each thread has a thread_data that is threadlocal. In it it has a mail_inbox that has a representative mail_outbox reference. 
The arena which is the task pool handler, or the mechanism that connects the threads with the tasks has the mail_outboxes that the 
threadlocal mail_inboxes reference. When a task is spawned through double spawning. The task is made opaque using masking aka making it 
a proxy. This proxy is then pushed to the mail_outbox of the slot/thread id (which is registered in the scheduler/arena). Now the thread 
can try to claim its proxy. But if it is busy for load balancing measures we also spawn the task in our own task pool open for stealing. 
It is now a question of which process is faster. If the mailbox route is taken the task is extracted from the proxy and it is executed. 
Now comes the cool part, the mailbox thread does not need to delete the obj as it is not its duty. A stealing thread will try to get the proxy 
but it will fail so it can delete the proxy. This might seem extra complicated but empirical evidence suggest this is lowering the latency of scheduling. 







